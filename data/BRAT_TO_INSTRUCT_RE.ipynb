{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db95fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from glob import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8855d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def follows_pattern(s):\n",
    "    # Define the regex pattern\n",
    "    pattern = r'^[TR]\\d+'\n",
    "    \n",
    "    # Use re.match to check if the string matches the pattern\n",
    "    if re.match(pattern, s):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f77f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorg_annfile(file):\n",
    "    new_file=[]\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "    skip = False\n",
    "    for i,line in enumerate(lines):\n",
    "        if i <= len(lines)-2:\n",
    "            if not follows_pattern(lines[i+1]):\n",
    "                new_file.append(line.strip()+' '+lines[i+1].strip()+'\\n')\n",
    "                skip = True\n",
    "            else:\n",
    "                if not skip:\n",
    "                    new_file.append(line)\n",
    "                else:\n",
    "                    skip = False\n",
    "                    pass\n",
    "                \n",
    "        else:\n",
    "            if not skip:\n",
    "                new_file.append(line)\n",
    "            else:\n",
    "                skip = False\n",
    "                pass\n",
    "    return new_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00846c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_brat_files(txt_path, ann_path):\n",
    "\n",
    "    with open(txt_path, \"r\") as txt_file:\n",
    "        text = txt_file.read()\n",
    "    \n",
    "    entities = {}\n",
    "    relations = []\n",
    "    \n",
    "    new_lines = reorg_annfile(ann_path)\n",
    "    for line in new_lines:\n",
    "        try:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if parts[0].startswith('T'):\n",
    "                try:\n",
    "                    ent_id, ent_info = parts[0], parts[1]\n",
    "                    ent_type, start, end = ent_info.split(' ')\n",
    "                    ent_type = ent_type.lower()\n",
    "                    entities[ent_id] = (ent_type, int(start), int(end))\n",
    "                except:\n",
    "\n",
    "                    print (new_lines)\n",
    "\n",
    "                    raise\n",
    "            elif parts[0].startswith('R'):\n",
    "                rel_id, rel_type, arg1, arg2 = [parts[0]]+parts[1].split(' ')\n",
    "                relations.append((rel_type, entities[arg1.split(':')[1]], entities[arg2.split(':')[1]]))\n",
    "        except:\n",
    "            continue\n",
    "    return text, entities, relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "796906a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_dict = {'hasAttr':0}\n",
    "\n",
    "def replace_entities_with_types(sent, ent1, ent2):\n",
    "    sent_text = str(sent)\n",
    "    replacements = []\n",
    "    \n",
    "    if ent1[1]>ent2[1]:\n",
    "        ent1,ent2 = ent2,ent1\n",
    "        \n",
    "    if ent2[1]>ent1[2]:\n",
    "        original_len = len(sent_text)\n",
    "        ent_type, start, end =ent1\n",
    "        #sent_text = sent_text[:start - sent.start_char]+f\"@{ent_type}$\"+sent_text[end - sent.start_char:]\n",
    "        sent_text = sent_text[:start - sent.start_char]+f\"@{sent_text[start - sent.start_char:end - sent.start_char]}$\"+sent_text[end - sent.start_char:]\n",
    "        modified_len = len(sent_text)\n",
    "\n",
    "        len_diff = original_len - modified_len\n",
    "        ent_type, start, end =ent2\n",
    "        #sent_text = sent_text[:start - sent.start_char - len_diff]+f\"@{ent_type}$\"+sent_text[end - sent.start_char- len_diff: ]\n",
    "        sent_text = sent_text[:start - sent.start_char - len_diff]+f\"@{sent_text[start - sent.start_char - len_diff:end - sent.start_char- len_diff]}$\"+sent_text[end - sent.start_char- len_diff: ]\n",
    "    else:\n",
    "\n",
    "        original_len = len(sent_text)\n",
    "        ent_type, start, end =ent1\n",
    "        #sent_text = sent_text[:start - sent.start_char]+f\"@{ent_type}$\"+sent_text[end - sent.start_char:]\n",
    "        sent_text = sent_text[:start - sent.start_char]+f\"@{sent_text[start - sent.start_char:end - sent.start_char]}$\"+sent_text[end - sent.start_char:]\n",
    "        modified_len = len(sent_text)\n",
    "\n",
    "        len_diff = original_len - modified_len +1\n",
    "        ent_type, start, end =ent2\n",
    "        #sent_text = sent_text[:start - sent.start_char - len_diff]+f\"@{ent_type}$\"+sent_text[end - sent.start_char- len_diff: ]\n",
    "        sent_text = sent_text[:start - sent.start_char - len_diff]+f\"@{sent_text[start - sent.start_char - len_diff:end - sent.start_char- len_diff]}$\"+sent_text[end - sent.start_char- len_diff: ]\n",
    "\n",
    "    return sent_text\n",
    "\n",
    "def sentence_relations(text,entities, relations, nlp):\n",
    "    all_entity_pairs = {'test': ['temporal', 'labvalue', 'reference_range', 'negation'],\n",
    "     'treatment': ['temporal', 'negation'],\n",
    "     'problem': ['bodyloc',\n",
    "      'negation',\n",
    "      'temporal',\n",
    "      'subject',\n",
    "      'severity',\n",
    "      'course',\n",
    "      'condition',\n",
    "      'uncertain'],\n",
    "     'drug': ['temporal',\n",
    "      'route',\n",
    "      'form',\n",
    "      'strength',\n",
    "      'frequency',\n",
    "      'dosage',\n",
    "      'duration',\n",
    "      'negation']}\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    sentence_relations = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        sent_relations = set()\n",
    "        sent_entities = []\n",
    "\n",
    "        # Check if entities are in the current sentence\n",
    "        for ent_id, ent in entities.items():\n",
    "            if ent[1] >= sent.start_char and ent[2] <= sent.end_char:\n",
    "                sent_entities.append(ent)\n",
    "                \n",
    "        # Check for existing relations in the current sentence\n",
    "        for rel in relations:\n",
    "            rel_type, ent1, ent2 = rel\n",
    "            if ent1 in sent_entities and ent2 in sent_entities:\n",
    "                sent_relations.add((ent1, ent2))\n",
    "                # Replace entity mentions with their types\n",
    "                sent_text = replace_entities_with_types(sent, ent1, ent2)\n",
    "                sentence_relations.append((ent1[0], ent2[0],text[ent1[1]:ent1[2]].replace('\\n',' '),text[ent2[1]:ent2[2]].replace('\\n',' '), sent_text.strip().replace('\\n',' '), 0))\n",
    "\n",
    "        #print (sent_relations)\n",
    "        # Check for negative relations in the current sentence\n",
    "        for ent1 in sent_entities:\n",
    "            for ent2 in sent_entities:\n",
    "                if ent1 != ent2 and (ent1, ent2) not in sent_relations and  (ent2, ent1) not in sent_relations:\n",
    "                    #print ((ent1, ent2))\n",
    "                    for e in all_entity_pairs.keys():\n",
    "                        if ent1[0] == e:\n",
    "                            if ent2[0] in all_entity_pairs[e]:\n",
    "                                #print (file)\n",
    "                                sent_text = replace_entities_with_types(sent, ent1, ent2)\n",
    "                                sentence_relations.append((ent1[0], ent2[0],text[ent1[1]:ent1[2]].replace('\\n',' '),text[ent2[1]:ent2[2]].replace('\\n',' '),  sent_text.strip().replace('\\n',' '), f\"1\"))\n",
    "\n",
    "    return sentence_relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c605861",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_dict = {'hasAttr':0}\n",
    "\n",
    "def group_relations(tuples_set):\n",
    "    grouped_tuples = {}\n",
    "    for pair in tuples_set:\n",
    "        key = pair[0]  # The first tuple of the pair\n",
    "        if key in grouped_tuples:\n",
    "            grouped_tuples[key].append(pair[1])\n",
    "        else:\n",
    "            grouped_tuples[key] = [pair[1]]\n",
    "            \n",
    "    # Sorting the values in each group in descending order based on the second value of the tuple\n",
    "    sorted_grouped_tuples = {}\n",
    "    for key, values in grouped_tuples.items():\n",
    "        sorted_grouped_tuples[key] = sorted(values, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sorted_grouped_tuples\n",
    "\n",
    "def replace_entities_with_types(sent, entities):\n",
    "    sent_text = str(sent)\n",
    "    if isinstance(entities, list):\n",
    "        for e in entities:\n",
    "            ent_type, start, end =e\n",
    "            sent_text = sent_text[:start - sent.start_char]+f'<span class=\"{ent_type}\">{sent_text[start - sent.start_char:end - sent.start_char]}</span>'+sent_text[end - sent.start_char:] \n",
    "    else:\n",
    "        ent_type, start, end =entities\n",
    "        sent_text = sent_text[:start - sent.start_char]+f'<span class=\"{ent_type}\">{sent_text[start - sent.start_char:end - sent.start_char]}</span>'+sent_text[end - sent.start_char:] \n",
    "    return sent_text\n",
    "\n",
    "def sentence_relations(text,entities, relations, nlp):\n",
    "    test_prompt = '''### Task:\n",
    "    Your task is to mark up modifier entities related to the entity marked with <span> tag in the input text.\n",
    "    \n",
    "    ### Entity Markup Guide:\n",
    "    Use <span class=\"labvalue\"> to denote a numeric value or a normal description of the result of a lab test.\n",
    "    Use <span class=\"reference_range\"> to denote the range or interval of values that are deemed as normal for a test in a healthy person.\n",
    "    Use <span class=\"negation\"> to denote the phrase that indicates the absence of an entity.\n",
    "    Use <span class=\"temporal\"> to denote a calendar date, time, or duration related to a test.\n",
    "\n",
    "    ### Input Text: {} <EOS>\n",
    "    ### Output Text:'''\n",
    "    \n",
    "    drug_prompt = '''### Task:\n",
    "    Your task is to mark up modifier entities related to the entity marked with <span> tag in the input text.\n",
    "\n",
    "    ### Entity Markup Guide:\n",
    "    Use <span class=\"form\"> to denote the form of drug.\n",
    "    Use <span class=\"frequency\"> to denote the frequency of taking a drug.\n",
    "    Use <span class=\"dosage\"> to denote the amount of active ingredient from the number of drugs prescribed.\n",
    "    Use <span class=\"duration\"> to denote the time period a patient should take a drug.\n",
    "    Use <span class=\"strength\"> to denote the amount of active ingredient in a given dosage form.\n",
    "    Use <span class=\"route\"> to denote the way by which a drug, fluid, poison, or other substance is taken into the body.\n",
    "    Use <span class=\"negation\"> to denote the phrase that indicates the absence of an entity.\n",
    "    Use <span class=\"temporal\"> to denote a calendar date, time, or duration related to a drug.\n",
    "\n",
    "    ### Input Text: {} <EOS>\n",
    "    ### Output Text:'''\n",
    "\n",
    "    problem_prompt = '''### Task:\n",
    "    Your task is to mark up modifier entities related to the entity marked with <span> tag in the input text.\n",
    "\n",
    "    ### Entity Markup Guide:\n",
    "    Use <span class=\"uncertain\"> to denote a measure of doubt.\n",
    "    Use <span class=\"condition\"> to denote a phrase that indicates the problems existing in a certain situation.\n",
    "    Use <span class=\"subject\"> to denote the person entity who is experiencing the disorder.\n",
    "    Use <span class=\"negation\"> to denote the phrase that indicates the absence of an entity.\n",
    "    Use <span class=\"bodyloc\"> to denote the location on the body where the observation is present.\n",
    "    Use <span class=\"severity\"> to denote the degree of intensity of a clinical condition.\n",
    "    Use <span class=\"temporal\"> to denote a calendar date, time, or duration related to a problem.\n",
    "    Use <span class=\"course\"> to denote the development or alteration of a problem.\n",
    "\n",
    "    ### Input Text: {} <EOS>\n",
    "    ### Output Text:'''\n",
    "    \n",
    "    treatment_prompt = '''### Task:\n",
    "    Your task is to mark up modifier entities related to the entity marked with <span> tag in the input text.\n",
    "\n",
    "    ### Entity Markup Guide:\n",
    "    Use <span class=\"temporal\"> to denote a calendar date, time, or duration related to a treatment.\n",
    "    Use <span class=\"negation\"> to denote the phrase that indicates the absence of an entity.\n",
    "\n",
    "    ### Input Text: {} <EOS>\n",
    "    ### Output Text:'''\n",
    "    df = pd.DataFrame(columns=['unprocessed', 'processed'])\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    sentence_relations = []\n",
    "\n",
    "    unprocessed = []\n",
    "    processed = []\n",
    "    for sent in sentences:\n",
    "        sent_relations = set()\n",
    "        sent_entities = []\n",
    "\n",
    "        # Check if entities are in the current sentence\n",
    "        for ent_id, ent in entities.items():\n",
    "            if ent[1] >= sent.start_char and ent[2] <= sent.end_char:\n",
    "                sent_entities.append(ent)\n",
    "                                \n",
    "        # Check for existing relations in the current sentence\n",
    "        for rel in relations:\n",
    "            rel_type, ent1, ent2 = rel\n",
    "            if ent1 in sent_entities and ent2 in sent_entities:\n",
    "                sent_relations.add((ent1, ent2))\n",
    "        sent_relations = group_relations(sent_relations)\n",
    "        \n",
    "        for main_entity, modifier_entities in sent_relations.items():\n",
    "            # Replace entity mentions with their types\n",
    "            modifier_sent_text = replace_entities_with_types(sent, modifier_entities).replace('\\n',' ')\n",
    "            main_entity_sent_text = replace_entities_with_types(sent, main_entity).replace('\\n',' ')\n",
    "            \n",
    "            if main_entity[0] == 'problem':\n",
    "                unprocessed.append(problem_prompt.format(main_entity_sent_text))\n",
    "                processed.append(modifier_sent_text+' <EOS>')\n",
    "            if main_entity[0] == 'drug':\n",
    "                unprocessed.append(drug_prompt.format(main_entity_sent_text))\n",
    "                processed.append(modifier_sent_text+' <EOS>')\n",
    "            if main_entity[0] == 'treatment':\n",
    "                unprocessed.append(treatment_prompt.format(main_entity_sent_text))\n",
    "                processed.append(modifier_sent_text+' <EOS>')\n",
    "            if main_entity[0] == 'test':\n",
    "                unprocessed.append(test_prompt.format(main_entity_sent_text))\n",
    "                processed.append(modifier_sent_text+' <EOS>')\n",
    "        # check for non-existing relations in current sentence\n",
    "        for entity in sent_entities:\n",
    "            if entity not in sent_relations and entity[0] in ['problem','treatment','test','drug']:\n",
    "                main_entity_sent_text = replace_entities_with_types(sent, entity).replace('\\n',' ')\n",
    "                sent_text = str(sent).replace('\\n',' ')\n",
    "                if entity[0] == 'problem':\n",
    "                    unprocessed.append(problem_prompt.format(main_entity_sent_text))\n",
    "                    processed.append(sent_text+' <EOS>')\n",
    "                if entity[0] == 'drug':\n",
    "                    unprocessed.append(drug_prompt.format(main_entity_sent_text))\n",
    "                    processed.append(sent_text+' <EOS>')\n",
    "                if entity[0] == 'treatment':\n",
    "                    unprocessed.append(treatment_prompt.format(main_entity_sent_text))\n",
    "                    processed.append(sent_text+' <EOS>')\n",
    "                if entity[0] == 'test':\n",
    "                    unprocessed.append(test_prompt.format(main_entity_sent_text))\n",
    "                    processed.append(sent_text+' <EOS>')\n",
    "    df = pd.concat([df, pd.DataFrame({'unprocessed': unprocessed, 'processed': processed})], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db1d6bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/data/yhu5/LLAMA2/data/CLAMP_data_reorg/splitted_dataset_RE/LLAMA2/dev/’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load the SpaCy model for sentence tokenization\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "    \n",
    "    txt = glob(f'./splitted_dataset_RE/dev/*.txt')\n",
    "\n",
    "    relation_types = []\n",
    "    entity_pairs = []\n",
    "    !mkdir /data/yhu5/LLAMA2/data/CLAMP_data_reorg/splitted_dataset_RE/LLAMA2/dev/\n",
    "    df = pd.DataFrame(columns=['unprocessed', 'processed'])\n",
    "    for txt_path in txt:\n",
    "        filename = txt_path.split('/')[-1].split('.')[0]\n",
    "        ann_path = f'./splitted_dataset_RE/dev/{filename}.ann'\n",
    "        output_file = f'./splitted_dataset_RE/LLAMA2/dev/{filename}.csv'\n",
    "\n",
    "        # Read the Brat files and extract the entities and relations\n",
    "        text, entities, relations = read_brat_files(txt_path, ann_path)\n",
    "        \n",
    "        #for relation in relations:\n",
    "        #    print (relation)\n",
    "\n",
    "        # Extract sentence relations\n",
    "        df = pd.concat([df, sentence_relations(text, entities, relations, nlp)])\n",
    "        \n",
    "    # Save the table to a CSV file\n",
    "    df.to_csv('./RE_dev.csv', index=False)\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a241f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_relations(filename, text,entities, relations, nlp):\n",
    "    test_prompt = '''### Task:\n",
    "    Your task is to mark up modifier entities related to the entity marked with <span> tag in the input text.\n",
    "    \n",
    "    ### Entity Markup Guide:\n",
    "    Use <span class=\"labvalue\"> to denote a numeric value or a normal description of the result of a lab test.\n",
    "    Use <span class=\"reference_range\"> to denote the range or interval of values that are deemed as normal for a test in a healthy person.\n",
    "    Use <span class=\"negation\"> to denote the phrase that indicates the absence of an entity.\n",
    "    Use <span class=\"temporal\"> to denote a calendar date, time, or duration related to a test.\n",
    "\n",
    "    ### Input Text: {} <EOS>\n",
    "    ### Output Text:'''\n",
    "    \n",
    "    drug_prompt = '''### Task:\n",
    "    Your task is to mark up modifier entities related to the entity marked with <span> tag in the input text.\n",
    "\n",
    "    ### Entity Markup Guide:\n",
    "    Use <span class=\"form\"> to denote the form of drug.\n",
    "    Use <span class=\"frequency\"> to denote the frequency of taking a drug.\n",
    "    Use <span class=\"dosage\"> to denote the amount of active ingredient from the number of drugs prescribed.\n",
    "    Use <span class=\"duration\"> to denote the time period a patient should take a drug.\n",
    "    Use <span class=\"strength\"> to denote the amount of active ingredient in a given dosage form.\n",
    "    Use <span class=\"route\"> to denote the way by which a drug, fluid, poison, or other substance is taken into the body.\n",
    "    Use <span class=\"negation\"> to denote the phrase that indicates the absence of an entity.\n",
    "    Use <span class=\"temporal\"> to denote a calendar date, time, or duration related to a drug.\n",
    "\n",
    "    ### Input Text: {} <EOS>\n",
    "    ### Output Text:'''\n",
    "\n",
    "    problem_prompt = '''### Task:\n",
    "    Your task is to mark up modifier entities related to the entity marked with <span> tag in the input text.\n",
    "\n",
    "    ### Entity Markup Guide:\n",
    "    Use <span class=\"uncertain\"> to denote a measure of doubt.\n",
    "    Use <span class=\"condition\"> to denote a phrase that indicates the problems existing in a certain situation.\n",
    "    Use <span class=\"subject\"> to denote the person entity who is experiencing the disorder.\n",
    "    Use <span class=\"negation\"> to denote the phrase that indicates the absence of an entity.\n",
    "    Use <span class=\"bodyloc\"> to denote the location on the body where the observation is present.\n",
    "    Use <span class=\"severity\"> to denote the degree of intensity of a clinical condition.\n",
    "    Use <span class=\"temporal\"> to denote a calendar date, time, or duration related to a problem.\n",
    "    Use <span class=\"course\"> to denote the development or alteration of a problem.\n",
    "\n",
    "    ### Input Text: {} <EOS>\n",
    "    ### Output Text:'''\n",
    "    \n",
    "    treatment_prompt = '''### Task:\n",
    "    Your task is to mark up modifier entities related to the entity marked with <span> tag in the input text.\n",
    "\n",
    "    ### Entity Markup Guide:\n",
    "    Use <span class=\"temporal\"> to denote a calendar date, time, or duration related to a treatment.\n",
    "    Use <span class=\"negation\"> to denote the phrase that indicates the absence of an entity.\n",
    "\n",
    "    ### Input Text: {} <EOS>\n",
    "    ### Output Text:'''\n",
    "    df = pd.DataFrame(columns=['file_name','sentence_idx','main_entity','unprocessed', 'processed'])\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    sentence_relations = []\n",
    "\n",
    "    unprocessed = []\n",
    "    processed = []\n",
    "    main_entities = []\n",
    "    index = []\n",
    "    for i,sent in enumerate(sentences):\n",
    "        #print (i,sent)\n",
    "        sent_relations = set()\n",
    "        sent_entities = []\n",
    "\n",
    "        # Check if entities are in the current sentence\n",
    "        for ent_id, ent in entities.items():\n",
    "            if ent[1] >= sent.start_char and ent[2] <= sent.end_char:\n",
    "                sent_entities.append(ent)\n",
    "                                \n",
    "        # Check for existing relations in the current sentence\n",
    "        for rel in relations:\n",
    "            rel_type, ent1, ent2 = rel\n",
    "            if ent1 in sent_entities and ent2 in sent_entities:\n",
    "                sent_relations.add((ent1, ent2))\n",
    "        sent_relations = group_relations(sent_relations)\n",
    "        \n",
    "        for main_entity, modifier_entities in sent_relations.items():\n",
    "            #print (main_entity[0])\n",
    "            main_entities.append(main_entity[0])\n",
    "            # Replace entity mentions with their types\n",
    "            modifier_sent_text = replace_entities_with_types(sent, modifier_entities).replace('\\n',' ')\n",
    "            main_entity_sent_text = replace_entities_with_types(sent, main_entity).replace('\\n',' ')\n",
    "            \n",
    "            if main_entity[0] == 'problem':\n",
    "                index.append(i)\n",
    "                unprocessed.append(problem_prompt.format(main_entity_sent_text))\n",
    "                processed.append(modifier_sent_text+' <EOS>')\n",
    "            if main_entity[0] == 'drug':\n",
    "                index.append(i)\n",
    "                unprocessed.append(drug_prompt.format(main_entity_sent_text))\n",
    "                processed.append(modifier_sent_text+' <EOS>')\n",
    "            if main_entity[0] == 'treatment':\n",
    "                index.append(i)\n",
    "                unprocessed.append(treatment_prompt.format(main_entity_sent_text))\n",
    "                processed.append(modifier_sent_text+' <EOS>')\n",
    "            if main_entity[0] == 'test':\n",
    "                index.append(i)\n",
    "                unprocessed.append(test_prompt.format(main_entity_sent_text))\n",
    "                processed.append(modifier_sent_text+' <EOS>')\n",
    "        # check for non-existing relations in current sentence\n",
    "        for entity in sent_entities:\n",
    "            if entity not in sent_relations and entity[0] in ['problem','treatment','test','drug']:\n",
    "                #print (entity[0])\n",
    "                main_entities.append(entity[0])\n",
    "                main_entity_sent_text = replace_entities_with_types(sent, entity).replace('\\n',' ')\n",
    "                sent_text = str(sent).replace('\\n',' ')\n",
    "                if entity[0] == 'problem':\n",
    "                    index.append(i)\n",
    "                    unprocessed.append(problem_prompt.format(main_entity_sent_text))\n",
    "                    processed.append(sent_text+' <EOS>')\n",
    "                if entity[0] == 'drug':\n",
    "                    index.append(i)\n",
    "                    unprocessed.append(drug_prompt.format(main_entity_sent_text))\n",
    "                    processed.append(sent_text+' <EOS>')\n",
    "                if entity[0] == 'treatment':\n",
    "                    index.append(i)\n",
    "                    unprocessed.append(treatment_prompt.format(main_entity_sent_text))\n",
    "                    processed.append(sent_text+' <EOS>')\n",
    "                if entity[0] == 'test':\n",
    "                    index.append(i)\n",
    "                    unprocessed.append(test_prompt.format(main_entity_sent_text))\n",
    "                    processed.append(sent_text+' <EOS>')\n",
    "    df = pd.concat([df, pd.DataFrame({'file_name':filename,'sentence_idx':index,'main_entity':main_entities,'unprocessed': unprocessed, 'processed': processed})], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4be5ef9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./reorg_datasets/test/i2b2_test/brat/discharge81.txt', './reorg_datasets/test/i2b2_test/brat/0457.txt', './reorg_datasets/test/i2b2_test/brat/0408.txt', './reorg_datasets/test/i2b2_test/brat/0360.txt', './reorg_datasets/test/i2b2_test/brat/progress111.txt', './reorg_datasets/test/i2b2_test/brat/progress78.txt', './reorg_datasets/test/i2b2_test/brat/progress117.txt', './reorg_datasets/test/i2b2_test/brat/0007.txt', './reorg_datasets/test/i2b2_test/brat/discharge68.txt', './reorg_datasets/test/i2b2_test/brat/progress28.txt', './reorg_datasets/test/i2b2_test/brat/progress53.txt', './reorg_datasets/test/i2b2_test/brat/discharge77.txt', './reorg_datasets/test/i2b2_test/brat/record-177.txt', './reorg_datasets/test/i2b2_test/brat/discharge95.txt', './reorg_datasets/test/i2b2_test/brat/progress105.txt', './reorg_datasets/test/i2b2_test/brat/progress87.txt', './reorg_datasets/test/i2b2_test/brat/0469.txt', './reorg_datasets/test/i2b2_test/brat/discharge50.txt', './reorg_datasets/test/i2b2_test/brat/discharge15.txt', './reorg_datasets/test/i2b2_test/brat/record-54.txt', './reorg_datasets/test/i2b2_test/brat/0375.txt', './reorg_datasets/test/i2b2_test/brat/discharge20.txt', './reorg_datasets/test/i2b2_test/brat/progress29.txt', './reorg_datasets/test/i2b2_test/brat/record-83.txt', './reorg_datasets/test/i2b2_test/brat/0207.txt', './reorg_datasets/test/i2b2_test/brat/progress86.txt', './reorg_datasets/test/i2b2_test/brat/record-21.txt', './reorg_datasets/test/i2b2_test/brat/discharge17_admin.txt', './reorg_datasets/test/i2b2_test/brat/discharge105.txt', './reorg_datasets/test/i2b2_test/brat/0049.txt', './reorg_datasets/test/i2b2_test/brat/discharge117.txt', './reorg_datasets/test/i2b2_test/brat/discharge97_admin.txt', './reorg_datasets/test/i2b2_test/brat/0038.txt', './reorg_datasets/test/i2b2_test/brat/progress67.txt', './reorg_datasets/test/i2b2_test/brat/progress70.txt', './reorg_datasets/test/i2b2_test/brat/discharge60.txt', './reorg_datasets/test/i2b2_test/brat/progress109_admin.txt', './reorg_datasets/test/i2b2_test/brat/discharge22.txt', './reorg_datasets/test/i2b2_test/brat/record-28.txt', './reorg_datasets/test/i2b2_test/brat/0264.txt', './reorg_datasets/test/i2b2_test/brat/0333.txt', './reorg_datasets/test/i2b2_test/brat/0300.txt', './reorg_datasets/test/i2b2_test/brat/0092.txt', './reorg_datasets/test/i2b2_test/brat/0390.txt', './reorg_datasets/test/i2b2_test/brat/discharge87.txt', './reorg_datasets/test/i2b2_test/brat/0261.txt', './reorg_datasets/test/i2b2_test/brat/record-24.txt', './reorg_datasets/test/i2b2_test/brat/record-35.txt', './reorg_datasets/test/i2b2_test/brat/progress73.txt', './reorg_datasets/test/i2b2_test/brat/progress56.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/i2b2_test’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/i2b2_test/html/’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/i2b2_test/html/test’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/i2b2_test/html/problem’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/i2b2_test/html/treatment’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/i2b2_test/html/drug’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3847\n",
      "['./reorg_datasets/test/MTSample_test/brat/166_20210112211715.txt', './reorg_datasets/test/MTSample_test/brat/sample_2757.txt', './reorg_datasets/test/MTSample_test/brat/sample_1295.txt', './reorg_datasets/test/MTSample_test/brat/sample_963.txt', './reorg_datasets/test/MTSample_test/brat/160_20210112211715.txt', './reorg_datasets/test/MTSample_test/brat/sample_364.txt', './reorg_datasets/test/MTSample_test/brat/sample_2755.txt', './reorg_datasets/test/MTSample_test/brat/sample_2790.txt', './reorg_datasets/test/MTSample_test/brat/sample_648.txt', './reorg_datasets/test/MTSample_test/brat/sample_207.txt', './reorg_datasets/test/MTSample_test/brat/sample_1246.txt', './reorg_datasets/test/MTSample_test/brat/sample_52.txt', './reorg_datasets/test/MTSample_test/brat/80_20210112211705.txt', './reorg_datasets/test/MTSample_test/brat/sample_1714.txt', './reorg_datasets/test/MTSample_test/brat/sample_402.txt', './reorg_datasets/test/MTSample_test/brat/85_20210112211705.txt', './reorg_datasets/test/MTSample_test/brat/sample_530.txt', './reorg_datasets/test/MTSample_test/brat/sample_2762.txt', './reorg_datasets/test/MTSample_test/brat/sample_635.txt', './reorg_datasets/test/MTSample_test/brat/done_sample_2791.txt', './reorg_datasets/test/MTSample_test/brat/140_20210112211712.txt', './reorg_datasets/test/MTSample_test/brat/sample_323.txt', './reorg_datasets/test/MTSample_test/brat/sample_2748.txt', './reorg_datasets/test/MTSample_test/brat/sample_1854.txt', './reorg_datasets/test/MTSample_test/brat/151_20210112211713.txt', './reorg_datasets/test/MTSample_test/brat/sample_1258.txt', './reorg_datasets/test/MTSample_test/brat/87_20210112211705.txt', './reorg_datasets/test/MTSample_test/brat/sample_343.txt', './reorg_datasets/test/MTSample_test/brat/4_20210112211655.txt', './reorg_datasets/test/MTSample_test/brat/157_20210112211713.txt', './reorg_datasets/test/MTSample_test/brat/sample_377.txt', './reorg_datasets/test/MTSample_test/brat/173_20210112211716.txt', './reorg_datasets/test/MTSample_test/brat/sample_1231.txt', './reorg_datasets/test/MTSample_test/brat/sample_641.txt', './reorg_datasets/test/MTSample_test/brat/64_20210112211703.txt', './reorg_datasets/test/MTSample_test/brat/sample_365.txt', './reorg_datasets/test/MTSample_test/brat/91_20210112211706.txt', './reorg_datasets/test/MTSample_test/brat/done_sample_2765.txt', './reorg_datasets/test/MTSample_test/brat/sample_2754.txt', './reorg_datasets/test/MTSample_test/brat/sample_1765.txt', './reorg_datasets/test/MTSample_test/brat/sample_1237.txt', './reorg_datasets/test/MTSample_test/brat/96_20210112211707.txt', './reorg_datasets/test/MTSample_test/brat/131_20210112211711.txt', './reorg_datasets/test/MTSample_test/brat/sample_1433.txt', './reorg_datasets/test/MTSample_test/brat/sample_1600.txt', './reorg_datasets/test/MTSample_test/brat/sample_1021.txt', './reorg_datasets/test/MTSample_test/brat/169_20210112211715.txt', './reorg_datasets/test/MTSample_test/brat/done_sample_1254.txt', './reorg_datasets/test/MTSample_test/brat/154_20210112211713.txt', './reorg_datasets/test/MTSample_test/brat/sample_2792.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/MTSample_test’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/MTSample_test/html/’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/MTSample_test/html/test’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/MTSample_test/html/problem’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/MTSample_test/html/treatment’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/MTSample_test/html/drug’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3882\n",
      "['./reorg_datasets/test/MIMIC3_test/brat/397.txt', './reorg_datasets/test/MIMIC3_test/brat/351.txt', './reorg_datasets/test/MIMIC3_test/brat/283.txt', './reorg_datasets/test/MIMIC3_test/brat/356.txt', './reorg_datasets/test/MIMIC3_test/brat/304.txt', './reorg_datasets/test/MIMIC3_test/brat/668.txt', './reorg_datasets/test/MIMIC3_test/brat/340.txt', './reorg_datasets/test/MIMIC3_test/brat/298.txt', './reorg_datasets/test/MIMIC3_test/brat/294.txt', './reorg_datasets/test/MIMIC3_test/brat/256.txt', './reorg_datasets/test/MIMIC3_test/brat/636.txt', './reorg_datasets/test/MIMIC3_test/brat/654.txt', './reorg_datasets/test/MIMIC3_test/brat/632.txt', './reorg_datasets/test/MIMIC3_test/brat/365.txt', './reorg_datasets/test/MIMIC3_test/brat/267.txt', './reorg_datasets/test/MIMIC3_test/brat/406.txt', './reorg_datasets/test/MIMIC3_test/brat/375.txt', './reorg_datasets/test/MIMIC3_test/brat/622.txt', './reorg_datasets/test/MIMIC3_test/brat/615.txt', './reorg_datasets/test/MIMIC3_test/brat/389.txt', './reorg_datasets/test/MIMIC3_test/brat/311.txt', './reorg_datasets/test/MIMIC3_test/brat/326.txt', './reorg_datasets/test/MIMIC3_test/brat/392.txt', './reorg_datasets/test/MIMIC3_test/brat/681.txt', './reorg_datasets/test/MIMIC3_test/brat/686.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/MIMIC3_test’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/MIMIC3_test/html/’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/MIMIC3_test/html/test’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/MIMIC3_test/html/problem’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/MIMIC3_test/html/treatment’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/MIMIC3_test/html/drug’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6446\n",
      "['./reorg_datasets/test/UTP_test/brat/83599508.txt', './reorg_datasets/test/UTP_test/brat/88977525.txt', './reorg_datasets/test/UTP_test/brat/82924358.txt', './reorg_datasets/test/UTP_test/brat/90721596.txt', './reorg_datasets/test/UTP_test/brat/56200121.txt', './reorg_datasets/test/UTP_test/brat/65560435.txt', './reorg_datasets/test/UTP_test/brat/70175786.txt', './reorg_datasets/test/UTP_test/brat/68024227.txt', './reorg_datasets/test/UTP_test/brat/56486908.txt', './reorg_datasets/test/UTP_test/brat/68761371.txt', './reorg_datasets/test/UTP_test/brat/66361476.txt', './reorg_datasets/test/UTP_test/brat/76279635.txt', './reorg_datasets/test/UTP_test/brat/59781389.txt', './reorg_datasets/test/UTP_test/brat/80721428.txt', './reorg_datasets/test/UTP_test/brat/84564650.txt', './reorg_datasets/test/UTP_test/brat/56630897.txt', './reorg_datasets/test/UTP_test/brat/91096314.txt', './reorg_datasets/test/UTP_test/brat/65852842.txt', './reorg_datasets/test/UTP_test/brat/81006564.txt', './reorg_datasets/test/UTP_test/brat/73422917.txt', './reorg_datasets/test/UTP_test/brat/73897744.txt', './reorg_datasets/test/UTP_test/brat/89771411.txt', './reorg_datasets/test/UTP_test/brat/68620633.txt', './reorg_datasets/test/UTP_test/brat/87402809.txt', './reorg_datasets/test/UTP_test/brat/85245389.txt', './reorg_datasets/test/UTP_test/brat/90217646.txt', './reorg_datasets/test/UTP_test/brat/89056477.txt', './reorg_datasets/test/UTP_test/brat/53791944.txt', './reorg_datasets/test/UTP_test/brat/73535972.txt', './reorg_datasets/test/UTP_test/brat/61176350.txt', './reorg_datasets/test/UTP_test/brat/84514494.txt', './reorg_datasets/test/UTP_test/brat/73198983.txt', './reorg_datasets/test/UTP_test/brat/72692765.txt', './reorg_datasets/test/UTP_test/brat/90045231.txt', './reorg_datasets/test/UTP_test/brat/79770036.txt', './reorg_datasets/test/UTP_test/brat/78617272.txt', './reorg_datasets/test/UTP_test/brat/56518065.txt', './reorg_datasets/test/UTP_test/brat/76553628.txt', './reorg_datasets/test/UTP_test/brat/68674595.txt', './reorg_datasets/test/UTP_test/brat/64636293.txt', './reorg_datasets/test/UTP_test/brat/71755461.txt', './reorg_datasets/test/UTP_test/brat/85237126.txt', './reorg_datasets/test/UTP_test/brat/61985517.txt', './reorg_datasets/test/UTP_test/brat/56126384.txt', './reorg_datasets/test/UTP_test/brat/81172552.txt', './reorg_datasets/test/UTP_test/brat/77026833.txt', './reorg_datasets/test/UTP_test/brat/70920196.txt', './reorg_datasets/test/UTP_test/brat/91799515.txt', './reorg_datasets/test/UTP_test/brat/78858856.txt', './reorg_datasets/test/UTP_test/brat/93418998.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/UTP_test’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/UTP_test/html/’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/UTP_test/html/test’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/UTP_test/html/problem’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/UTP_test/html/treatment’: File exists\n",
      "mkdir: cannot create directory ‘./splitted_dataset_RE/test/UTP_test/html/drug’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6465\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load the SpaCy model for sentence tokenization\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "    \n",
    "    for dataset in ['i2b2_test','MTSample_test','MIMIC3_test','UTP_test']:\n",
    "        if dataset=='MIMIC3_test':\n",
    "            txt = glob(f'./reorg_datasets/test/{dataset}/brat/*.txt')\n",
    "            print (txt)\n",
    "            relation_types = []\n",
    "            entity_pairs = []\n",
    "            df = pd.DataFrame(columns=['file_name','sentence_idx','main_entity','unprocessed', 'processed'])\n",
    "            for txt_path in txt:\n",
    "                filename = txt_path.split('/')[-1].split('.')[0]\n",
    "                txt_path = f'./individual_tests/mimic3_test/updated_50_brat/{filename}.txt'\n",
    "                ann_path = f'./individual_tests/mimic3_test/updated_50_brat/{filename}.ann'\n",
    "\n",
    "                # Read the Brat files and extract the entities and relations\n",
    "                text, entities, relations = read_brat_files(txt_path, ann_path)\n",
    "\n",
    "                # Extract sentence relations\n",
    "                df = pd.concat([df, sentence_relations(filename, text, entities, relations, nlp)])\n",
    "\n",
    "            os.system(f'mkdir ./splitted_dataset_RE/test')\n",
    "            os.system(f'mkdir ./splitted_dataset_RE/test/{dataset}')\n",
    "            os.system(f'mkdir ./splitted_dataset_RE/test/{dataset}/html/')\n",
    "            os.system(f'mkdir ./splitted_dataset_RE/test/{dataset}/html/test')\n",
    "            os.system(f'mkdir ./splitted_dataset_RE/test/{dataset}/html/problem')\n",
    "            os.system(f'mkdir ./splitted_dataset_RE/test/{dataset}/html/treatment')\n",
    "            os.system(f'mkdir ./splitted_dataset_RE/test/{dataset}/html/drug')\n",
    "\n",
    "            print (len(df))\n",
    "            for i, (index, row) in enumerate(df.iterrows()):\n",
    "                main_entity = row['main_entity']\n",
    "                #print (row)\n",
    "                with open(f'./splitted_dataset_RE/test/{dataset}/html/{main_entity}/{i}.html','w') as f:\n",
    "                    processed = row['processed'].split('<EOS>')[0]\n",
    "                    f.write(processed)\n",
    "\n",
    "            df.to_csv(f'./RE_{dataset}.csv',index=False)\n",
    "        else:\n",
    "            txt = glob(f'./reorg_datasets/test/{dataset}/brat/*.txt')\n",
    "            print (txt)\n",
    "            relation_types = []\n",
    "            entity_pairs = []\n",
    "            df = pd.DataFrame(columns=['file_name','sentence_idx','main_entity','unprocessed', 'processed'])\n",
    "            for txt_path in txt:\n",
    "                filename = txt_path.split('/')[-1].split('.')[0]\n",
    "                ann_path = f'./reorg_datasets/test/{dataset}/brat/{filename}.ann'\n",
    "\n",
    "                # Read the Brat files and extract the entities and relations\n",
    "                text, entities, relations = read_brat_files(txt_path, ann_path)\n",
    "\n",
    "                # Extract sentence relations\n",
    "                df = pd.concat([df, sentence_relations(filename, text, entities, relations, nlp)])\n",
    "\n",
    "            os.system(f'mkdir ./splitted_dataset_RE/test')\n",
    "            os.system(f'mkdir ./splitted_dataset_RE/test/{dataset}')\n",
    "            os.system(f'mkdir ./splitted_dataset_RE/test/{dataset}/html/')\n",
    "            os.system(f'mkdir ./splitted_dataset_RE/test/{dataset}/html/test')\n",
    "            os.system(f'mkdir ./splitted_dataset_RE/test/{dataset}/html/problem')\n",
    "            os.system(f'mkdir ./splitted_dataset_RE/test/{dataset}/html/treatment')\n",
    "            os.system(f'mkdir ./splitted_dataset_RE/test/{dataset}/html/drug')\n",
    "\n",
    "            print (len(df))\n",
    "            for i, (index, row) in enumerate(df.iterrows()):\n",
    "                main_entity = row['main_entity']\n",
    "                #print (row)\n",
    "                with open(f'./splitted_dataset_RE/test/{dataset}/html/{main_entity}/{i}.html','w') as f:\n",
    "                    processed = row['processed'].split('<EOS>')[0]\n",
    "                    f.write(processed)\n",
    "\n",
    "            df.to_csv(f'./RE_{dataset}.csv',index=False)\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e59676",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "llama2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
